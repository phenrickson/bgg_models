[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting Upcoming Board Games",
    "section": "",
    "text": "I use historical data from BoardGameGeek (BGG) to train a number of predictive models for community ratings. I first classify games based on their probability of achieving a minimum number of ratings on BGG. I then estimate each game’s complexity (average weight) in order to predicts its number of user ratings and average rating. I then use these estimates to predict the expected Geek Rating.\n\n\nThe following table displays predicted BGG outcomes for games that are expected to achieve at least 25 user ratings.\n\n\n\n\n\n\n\n\n\n\n\n\nThis table displays predicted probabilities for whether games will achieve enough ratings (25) to be assigned a Geek Rating"
  },
  {
    "objectID": "index.html#upcoming-games",
    "href": "index.html#upcoming-games",
    "title": "Predicting Upcoming Board Games",
    "section": "",
    "text": "The following table displays predicted BGG outcomes for games that are expected to achieve at least 25 user ratings."
  },
  {
    "objectID": "index.html#hurdle",
    "href": "index.html#hurdle",
    "title": "Predicting Upcoming Board Games",
    "section": "",
    "text": "This table displays predicted probabilities for whether games will achieve enough ratings (25) to be assigned a Geek Rating"
  },
  {
    "objectID": "predictions.html",
    "href": "predictions.html",
    "title": "Pipeline",
    "section": "",
    "text": "Show the code\n# Load required packages\nlibrary(dplyr)\nlibrary(bggUtils)\nlibrary(tidymodels)\nlibrary(vetiver)\nlibrary(targets)\nlibrary(qs2)\nlibrary(reactable)\nlibrary(fs)\n\n# Load source code\ntar_source(\"src\")\n\n# Project settings\nconfig &lt;- config::get()\n\n# Authenticate to GCS\nauthenticate_to_gcs()\nShow the code\ntargets::tar_visnetwork(targets_only = T)"
  },
  {
    "objectID": "predictions.html#load-models",
    "href": "predictions.html#load-models",
    "title": "Pipeline",
    "section": "1 Load Models",
    "text": "1 Load Models\n\n\nShow the code\n#|\n# Load objects from pipeline\ntar_load(hurdle_threshold)\n\n# Create model board connection\nmodel_board &lt;- gcs_model_board(bucket = config$bucket, prefix = config$board)\n\n# Load model metadata\ntar_load(averageweight_vetiver)\ntar_load(average_vetiver)\ntar_load(usersrated_vetiver)\ntar_load(hurdle_vetiver)\n\n# load models\naverageweight_fit &lt;- pin_read_model(model_board, averageweight_vetiver)\naverage_fit &lt;- pin_read_model(model_board, average_vetiver)\nusersrated_fit &lt;- pin_read_model(model_board, usersrated_vetiver)\nhurdle_fit &lt;- pin_read_model(model_board, hurdle_vetiver)\n\n\n\n\nShow the code\n# Function to extract model metadata\nextract_model_metadata &lt;- function(model_name, vetiver_obj, board) {\n    tibble(\n        bucket = board$bucket,\n        path = board$prefix,\n        model_name = model_name,\n        hash = vetiver_obj$hash,\n        version = vetiver_obj$version\n    )\n}\n\n# Create a table with model metadata\nmodel_metadata &lt;- bind_rows(\n    extract_model_metadata(\"averageweight\", averageweight_vetiver, model_board),\n    extract_model_metadata(\"average\", average_vetiver, model_board),\n    extract_model_metadata(\"usersrated\", usersrated_vetiver, model_board),\n    extract_model_metadata(\"hurdle\", hurdle_vetiver, model_board)\n)\n\n# Display the model metadata table\nmodel_metadata |&gt;\n    knitr::kable(\n        caption = \"Model Metadata\",\n        col.names = c(\"Bucket\", \"Prefix\", \"Model\", \"Hash\", \"Version\"),\n        format = \"markdown\"\n    )\n\n\n\nModel Metadata\n\n\n\n\n\n\n\n\n\nBucket\nPrefix\nModel\nHash\nVersion\n\n\n\n\nbgg_models\ndev/model/\naverageweight\n79313d79c1944fc8\n20250304T221625Z-79313\n\n\nbgg_models\ndev/model/\naverage\n66c6d76df420a7cf\n20250304T221529Z-66c6d\n\n\nbgg_models\ndev/model/\nusersrated\n06e70d3dd6bb333a\n20250304T221420Z-06e70\n\n\nbgg_models\ndev/model/\nhurdle\n9de9a43cd3d69c74\n20250304T221240Z-9de9a"
  },
  {
    "objectID": "predictions.html#load-games",
    "href": "predictions.html#load-games",
    "title": "Pipeline",
    "section": "2 Load Games",
    "text": "2 Load Games\n\n\nShow the code\nlist_gcs_objs = function(\n    obj = \"raw/objects/games\",\n    bucket = \"bgg_data\",\n    prefix = \"raw/objects/games\",\n    versions = T,\n    detail = \"full\"\n) {\n    googleCloudStorageR::gcs_list_objects(\n        bucket = bucket,\n        prefix = prefix,\n        versions = T,\n        detail = detail\n    ) |&gt;\n        filter(name == obj)\n}\n\n# get details of games objects from gcp\ngames_objs = list_gcs_objs() |&gt;\n    select(bucket, name, generation, size, updated) |&gt;\n    arrange(desc(updated))\n\n# ids for each game\ngames_generations = unique(games_objs$generation)\n\n# show most recent games objects\ngames_objs |&gt;\n    head(10) |&gt;\n    knitr::kable(\n        caption = \"Games\",\n        format = \"markdown\"\n    )\n\n\n\nGames\n\n\n\n\n\n\n\n\n\nbucket\nname\ngeneration\nsize\nupdated\n\n\n\n\nbgg_data\nraw/objects/games\n1742489667404929\n76.7 Mb\n2025-03-20 16:54:27\n\n\nbgg_data\nraw/objects/games\n1741629678449522\n73.6 Mb\n2025-03-10 18:01:18\n\n\nbgg_data\nraw/objects/games\n1740875193707566\n73.5 Mb\n2025-03-02 00:26:33\n\n\nbgg_data\nraw/objects/games\n1732133366577344\n72.1 Mb\n2024-11-20 20:09:26\n\n\nbgg_data\nraw/objects/games\n1730333447804461\n71.8 Mb\n2024-10-31 00:10:47\n\n\nbgg_data\nraw/objects/games\n1728676924892464\n71.5 Mb\n2024-10-11 20:02:04\n\n\nbgg_data\nraw/objects/games\n1726869993009335\n71.2 Mb\n2024-09-20 22:06:33\n\n\nbgg_data\nraw/objects/games\n1724967426023865\n70.9 Mb\n2024-08-29 21:37:06\n\n\nbgg_data\nraw/objects/games\n1723671710966893\n70.8 Mb\n2024-08-14 21:41:51\n\n\nbgg_data\nraw/objects/games\n1721888393714463\n70.5 Mb\n2024-07-25 06:19:53\n\n\n\n\n\n\n\nShow the code\ni = 1\n# most recent batch\ngames = get_games_from_gcp(\n    bucket = \"bgg_data\",\n    generation = games_generations[i]\n)\n\n# previous batch of games\nprevious_games = get_games_from_gcp(\n    bucket = \"bgg_data\",\n    generation = games_generations[i + 1]\n)\n\n# Prepare games with preprocessor\nprepared_games &lt;- prepare_games(games)\n\n\nNumber of games by yearpublished in recent and upcoming years.\n\n\nShow the code\n# Get valid years from targets to determine upcoming games\ntar_load(valid_predictions)\n\nvalid_years &lt;- valid_predictions |&gt;\n    summarize(min_year = min(yearpublished), max_year = max(yearpublished))\n\n# Filter to upcoming games (games published after the validation period)\nend_valid_year &lt;- valid_years$max_year\n\n# filter to only upcoming games\nupcoming_games &lt;- prepared_games |&gt;\n    filter(yearpublished &gt; end_valid_year)\n\n# count games by year\nupcoming_games |&gt;\n    group_by(yearpublished) |&gt;\n    count() |&gt;\n    mutate(yearpublished = as.factor(yearpublished)) |&gt;\n    gt::gt()\n\n\n\n\n\n  \n    \n      n\n    \n  \n  \n    \n      2024\n    \n    5460\n    \n      2025\n    \n    2421\n    \n      2026\n    \n    167\n    \n      2027\n    \n    3\n    \n      2028\n    \n    4\n  \n  \n  \n\n\n\n\nNew and upcoming games in most recent batch.\n\n\nShow the code\nupcoming_games_new = games |&gt;\n    anti_join(previous_games, by = join_by(game_id)) |&gt;\n    inner_join(upcoming_games, by = join_by(game_id)) |&gt;\n    bggUtils:::unnest_info() |&gt;\n    select(game_id, name, yearpublished) |&gt;\n    mutate(first_time_prediction = T)\n\nupcoming_games_new |&gt;\n    select(game_id, name, yearpublished) |&gt;\n    arrange(desc(yearpublished)) |&gt;\n    mutate(game_id = as.factor(game_id)) |&gt;\n    reactable::reactable()"
  },
  {
    "objectID": "predictions.html#predictions",
    "href": "predictions.html#predictions",
    "title": "Pipeline",
    "section": "3 Predictions",
    "text": "3 Predictions\nPredict games with models.\n\n\nShow the code\n# Generate predictions for upcoming games\npredictions &lt;- upcoming_games |&gt;\n    impute_averageweight(\n        model = averageweight_fit\n    ) |&gt;\n    predict_hurdle(\n        model = hurdle_fit,\n        threshold = hurdle_threshold\n    ) |&gt;\n    predict_bayesaverage(\n        average_model = average_fit,\n        usersrated_model = usersrated_fit\n    )\n\n# Add a flag for first-time predictions\npredictions &lt;- predictions |&gt;\n    left_join(\n        upcoming_games_new |&gt; select(game_id, first_time_prediction),\n        by = join_by(game_id)\n    ) |&gt;\n    mutate(first_time_prediction = replace_na(first_time_prediction, FALSE))\n\n\nView predictions for games appearing for the first time\n\n\nShow the code\npredictions |&gt;\n    filter(first_time_prediction == T) |&gt;\n    select(yearpublished, game_id, name, starts_with(\".pred\")) |&gt;\n    mutate(across(\n        starts_with(\".pred_\") & where(is.numeric),\n        round,\n        digits = 2\n    )) |&gt;\n    rename_with(~ gsub(\"^.pred_\", \"\", .), starts_with(\".pred_\")) |&gt;\n    select(yearpublished, game_id, name, contains(\"hurdle\"), everything()) |&gt;\n    arrange(desc(bayesaverage)) |&gt;\n    mutate(\n        game_id = as.factor(game_id),\n        yearpublished = as.factor(yearpublished)\n    ) |&gt;\n    reactable::reactable()"
  },
  {
    "objectID": "predictions.html#save-predictions",
    "href": "predictions.html#save-predictions",
    "title": "Pipeline",
    "section": "4 Save Predictions",
    "text": "4 Save Predictions\n\n\nShow the code\n# Create directory if it doesn't exist\nfs::dir_create(\"data/processed\", recurse = TRUE)\n\n# trim down predictions\npredictions_out =\n    predictions |&gt;\n    select(yearpublished, game_id, name, starts_with(\".pred_\"))\n\n# Save locally first\nlocal_board = pins::board_folder(\"data/processed\")\npins::pin_write(local_board, predictions_out, name = \"predictions\")\n\n# Save to Google Cloud Storage\n# Create a GCS board connection for predictions\ngcs_pred_board &lt;- pins::board_gcs(\n    bucket = config$bucket,\n    prefix = \"data/\",\n    versioned = T\n)\n\n# Pin predictions to GCS\npins::pin_write(gcs_pred_board, predictions_out, name = \"predictions\")\n\n# Print confirmation message\ncat(\n    \"Predictions saved to GCS bucket:\",\n    config$bucket,\n    \"with prefix:\",\n    paste0(config$board, \"/predictions\"),\n    \"\\n\"\n)\n\n\nPredictions saved to GCS bucket: bgg_models with prefix: dev/model//predictions"
  },
  {
    "objectID": "methodology.html",
    "href": "methodology.html",
    "title": "Methodology",
    "section": "",
    "text": "The goal of this project was to train models to predict new and upcoming releases once information about them becomes available on BoardGameGeek. How do you train a model to predict new releases?\nI use historical data from BoardGameGeek (BGG) to train a number of predictive models for community ratings. I train a series of models for this purpose.\n\nI first classify games based on their probability of achieving a minimum number of ratings on BGG.\nI then estimate each game’s complexity (average weight) in order to predicts its number of user ratings and average rating.\nI then use these estimates to predict the expected Geek Rating.\n\n\n\nI develop and train models using a training-validation approach based around the year in which games were published. I create a training set of games published prior to 2022 and evaluated its performance in predicting games published from 2022 to 2023.\n\n\n\n\n\ngraph LR\n    %% Data Sources and Storage\n    BGG[BoardGameGeek API] --&gt;|Data Collection| GCS[(Google Cloud Storage)]\n    GCS --&gt;|Data Loading| PrepData[Preprocessed Games Data]\n    \n    %% Data Splitting\n    PrepData --&gt;|Time-based Split| DataSets[Training/Validation/Testing Sets]\n    \n    %% Main Model Components\n    DataSets --&gt;|Training| Models[Model Training]\n    \n    %% Model Types\n    Models --&gt; HurdleModel[Hurdle Model&lt;br&gt;Classification]\n    Models --&gt; WeightModel[Complexity Model&lt;br&gt;Regression]\n    Models --&gt; RatingModels[Rating Models&lt;br&gt;Regression]\n    \n    %% Evaluation and Deployment\n    HurdleModel --&gt; Evaluation[Model Evaluation]\n    WeightModel --&gt; Evaluation\n    RatingModels --&gt; Evaluation\n    \n    Evaluation --&gt; Tracking[Performance Tracking]\n    Evaluation --&gt; Deployment[Model Deployment]\n    \n    %% Styling\n    classDef storage fill:#f9f,stroke:#333,stroke-width:2px;\n    classDef model fill:#bbf,stroke:#333,stroke-width:1px;\n    classDef data fill:#dfd,stroke:#333,stroke-width:1px;\n    \n    class GCS,VetiverModels storage;\n    class HurdleModel,WeightModel,RatingModels,Models model;\n    class PrepData,DataSets,NewGames,Predictions data;\n\n\n\n\n\n\n\n\n\nI train and evaluate the models primarily on games that have achieved at least 25 ratings, as this is the required number of user ratings for a game to receive a Geek rating.\nThe majority of games on BoardGameGeek do not actually receive this number of ratings, especially in recent years as the number of games published has dramatically increased. My main aim is to predict games that are set to be published and available to purchase, and many of the over 100k games on BGG have not been been published or distributed.\nIdeally, I would have a variable that measures this directly, but I instead rely on achieving a minimum number of ratings as a heuristic.\n\n\nShow the code\nplot_games_by_split = function(data) {\n    plot_data =\n        data |&gt;\n        bggUtils:::unnest_outcomes() |&gt;\n        inner_join(\n            data |&gt;\n                bggUtils:::unnest_info()\n        ) |&gt;\n        add_hurdle() |&gt;\n        mutate(\n            yearpublished = case_when(\n                yearpublished &lt; 1950 ~ 1949,\n                TRUE ~ yearpublished\n            )\n        ) |&gt;\n        mutate(\n            hurdle = case_when(\n                hurdle == 'yes' ~ '&gt;25 ratings',\n                hurdle == 'no' ~ '&lt;25 ratings'\n            )\n        ) |&gt;\n        group_by(yearpublished, hurdle) |&gt;\n        count()\n\n    plot_data |&gt;\n        ggplot(aes(x = yearpublished, y = n, fill = hurdle)) +\n        geom_col() +\n        scale_color_manual(values = c(\"grey60\", \"navy\"))\n}\n\nplot_games_by_split(games) +\n    scale_fill_manual(values = c('coral', 'navy'))\n\n\n\n\n\n\n\n\n\nIn predicting new games, I first use a hurdle model to predict whether games are expected to receive enough ratings to be assigned a Geek rating (25 ratings). This model is trained on the full universe of games, whereas the others are trained on games with at least 25 ratings.\nThe model for predicting the average weight is trained on games that have received 25 ratings and have also received at least 5 votes on their complexity.\n\n\nShow the code\nbind_rows(\n    as_tibble(hurdle_fit$metadata$user$data) |&gt;\n        group_by(model = 'hurdle') |&gt;\n        count(),\n    as_tibble(average_fit$metadata$user$data) |&gt;\n        group_by(model = 'average') |&gt;\n        count(),\n    as_tibble(averageweight_fit$metadata$user$data) |&gt;\n        group_by(model = 'averageweight') |&gt;\n        count(),\n    as_tibble(usersrated_fit$metadata$user$data) |&gt;\n        group_by(model = 'usersrated') |&gt;\n        count()\n) |&gt;\n    ungroup() |&gt;\n    rename(games = n) |&gt;\n    arrange(desc(games)) |&gt;\n    gt::gt()\n\n\n\n\n\n  \n    \n      model\n      games\n    \n  \n  \n    hurdle\n103528\n    average\n27812\n    usersrated\n27812\n    averageweight\n17315\n  \n  \n  \n\n\n\n\n\n\n\nHow did the models perform in predicting games? I evaluated the model on games published in 2022 and 2023.\n\n\nShow the code\nvalid_predictions |&gt;\n    group_by(yearpublished) |&gt;\n    count() |&gt;\n    ungroup() |&gt;\n    gt::gt()\n\n\n\n\n\n  \n    \n      yearpublished\n      n\n    \n  \n  \n    2022\n5053\n    2023\n5205\n  \n  \n  \n\n\n\n\n\n\nI first used the hurdle model to classify whether games would receive at least 25 ratings.\nI set the probability threshold at 0.16 - this was the point that maximized the F2 score and minimized false negatives. For the purpose of the hurdle model, including a game that did not receive a Geek rating is much worse than missing a game that did. The main purpose of the model is filter out low-quality games.\n\nThresholdMetricsConfusion MatrixFalse NegativesFalse Positives\n\n\n\n\nShow the code\nvalid_predictions |&gt;\n    ggplot(aes(x = .pred_hurdle_yes, fill = hurdle)) +\n    geom_density(alpha = 0.5) +\n    scale_color_manual() +\n    theme(legend.title = element_text()) +\n    xlab(\"Pr(User Ratings &gt;= 25)\") +\n    scale_fill_manual(values = c(\"coral\", \"navy\")) +\n    guides(fill = guide_legend(title = 'User Ratings &gt;=25')) +\n    geom_vline(xintercept = hurdle_threshold, linetype = 'dashed')\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nprob_metrics = metric_set(yardstick::roc_auc, yardstick::pr_auc)\n\nprob_hurdle_metrics =\n    valid_predictions |&gt;\n    group_by(outcome = 'hurdle') |&gt;\n    prob_metrics(truth = hurdle, .pred_hurdle_yes, event_level = 'second')\n\nvalid_hurdle_metrics |&gt;\n    bind_rows(prob_hurdle_metrics) |&gt;\n    gt::gt() |&gt;\n    gt::tab_options(quarto.disable_processing = T) |&gt;\n    gt::fmt_number(columns = c(\".estimate\"), decimals = 3) |&gt;\n    gtExtras::gt_theme_espn()\n\n\n\n\n\n  \n    \n      outcome\n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    hurdle\nbal_accuracy\nbinary\n0.763\n    hurdle\nkap\nbinary\n0.424\n    hurdle\nmcc\nbinary\n0.476\n    hurdle\nf1_meas\nbinary\n0.636\n    hurdle\nf2_meas\nbinary\n0.768\n    hurdle\nprecision\nbinary\n0.494\n    hurdle\nrecall\nbinary\n0.891\n    hurdle\nj_index\nbinary\n0.526\n    hurdle\nroc_auc\nbinary\n0.861\n    hurdle\npr_auc\nbinary\n0.737\n  \n  \n  \n\n\n\n\n\n\n\n\nShow the code\nvalid_predictions |&gt;\n    conf_mat(hurdle, .pred_hurdle_class) |&gt;\n    autoplot(type = 'heatmap')\n\n\n\n\n\n\n\n\n\n\n\nWhich games that the model classified as “no” received 25+ ratings?\n\n\nShow the code\nvalid_predictions |&gt;\n    filter(.pred_hurdle_class == 'no') |&gt;\n    filter(usersrated &gt;= 25) |&gt;\n    select(yearpublished, game_id, name, usersrated, bayesaverage) |&gt;\n    arrange(desc(usersrated)) |&gt;\n    mutate(bayesaverage = round(bayesaverage, 3)) |&gt;\n    mutate(across(all_of(c(\"yearpublished\", \"game_id\")), ~ as.factor(.x))) |&gt;\n    DT::datatable()\n\n\n\n\n\n\n\n\nWhich games that the model classified as “yes” did not receive 25+ ratings?\n\n\nShow the code\nvalid_predictions |&gt;\n    filter(.pred_hurdle_class == 'yes') |&gt;\n    filter(usersrated &lt; 25) |&gt;\n    select(yearpublished, game_id, name, usersrated, .pred_bayesaverage) |&gt;\n    arrange(desc(usersrated)) |&gt;\n    mutate(.pred_bayesaverage = round(.pred_bayesaverage, 3)) |&gt;\n    mutate(across(all_of(c(\"yearpublished\", \"game_id\")), ~ as.factor(.x))) |&gt;\n    DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\n\nI evaluate predictions for the average weight rating, the average rating, the number of user ratings, and the Geek rating.\n\nGames with &gt;25 RatingsGames with &lt;25 Ratings\n\n\n\n\nShow the code\nplot_hurdle_yes\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplot_hurdle_no\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ntargets_tracking_details(metrics = valid_metrics, details = details) |&gt;\n    select(\n        model,\n        minratings,\n        outcome,\n        any_of(c(\"rmse\", \"mae\", \"mape\", \"rsq\", \"ccc\"))\n    ) |&gt;\n    filter(minratings == 25) |&gt;\n    select(minratings, everything()) |&gt;\n    gt::gt() |&gt;\n    gt::tab_options(quarto.disable_processing = T) |&gt;\n    gtExtras::gt_theme_espn()\n\n\n\n\n\n  \n    \n      minratings\n      model\n      outcome\n      rmse\n      mae\n      mape\n      rsq\n      ccc\n    \n  \n  \n    25\nglmnet\naverage\n0.675\n0.498\n7.374\n0.294\n0.487\n    25\nlightgbm\naverageweight\n0.437\n0.336\n18.019\n0.706\n0.827\n    25\nglmnet+glmnet\nbayesaverage\n0.285\n0.159\n2.647\n0.430\n0.649\n    25\nglmnet\nusersrated\n1941.387\n446.031\n154.763\n0.122\n0.335\n  \n  \n  \n\n\n\n\nWhat were the model’s top predictions in the validation set?\n\n\nShow the code\nvalid_predictions |&gt;\n    filter(.pred_hurdle_class == 'yes') |&gt;\n    select(-starts_with(\".pred_hurdle\")) |&gt;\n    slice_max(.pred_bayesaverage, n = 150, with_ties = F) |&gt;\n    predictions_dt(\n        games = games,\n        lazy_load = TRUE,\n        pageLength = 10\n    ) |&gt;\n    add_colors()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich features were influential for predicting each BGG outcome?\n\n\nShow the code\naverage_plot =\n    average_fit |&gt;\n    extract_vetiver_features() |&gt;\n    plot_model_features() +\n    labs(title = 'Average Rating')\n\naverageweight_plot =\n    averageweight_fit |&gt;\n    extract_vetiver_features() |&gt;\n    plot_model_features() +\n    labs(title = 'Average Weight')\n\nusersrated_plot =\n    usersrated_fit |&gt;\n    extract_vetiver_features() |&gt;\n    plot_model_features() +\n    labs(title = 'Users Rated')\n\n\n\nAverage WeightAverageUsers Rated"
  },
  {
    "objectID": "methodology.html#methodology",
    "href": "methodology.html#methodology",
    "title": "Methodology",
    "section": "",
    "text": "I develop and train models using a training-validation approach based around the year in which games were published. I create a training set of games published prior to 2022 and evaluated its performance in predicting games published from 2022 to 2023.\n\n\n\n\n\ngraph LR\n    %% Data Sources and Storage\n    BGG[BoardGameGeek API] --&gt;|Data Collection| GCS[(Google Cloud Storage)]\n    GCS --&gt;|Data Loading| PrepData[Preprocessed Games Data]\n    \n    %% Data Splitting\n    PrepData --&gt;|Time-based Split| DataSets[Training/Validation/Testing Sets]\n    \n    %% Main Model Components\n    DataSets --&gt;|Training| Models[Model Training]\n    \n    %% Model Types\n    Models --&gt; HurdleModel[Hurdle Model&lt;br&gt;Classification]\n    Models --&gt; WeightModel[Complexity Model&lt;br&gt;Regression]\n    Models --&gt; RatingModels[Rating Models&lt;br&gt;Regression]\n    \n    %% Evaluation and Deployment\n    HurdleModel --&gt; Evaluation[Model Evaluation]\n    WeightModel --&gt; Evaluation\n    RatingModels --&gt; Evaluation\n    \n    Evaluation --&gt; Tracking[Performance Tracking]\n    Evaluation --&gt; Deployment[Model Deployment]\n    \n    %% Styling\n    classDef storage fill:#f9f,stroke:#333,stroke-width:2px;\n    classDef model fill:#bbf,stroke:#333,stroke-width:1px;\n    classDef data fill:#dfd,stroke:#333,stroke-width:1px;\n    \n    class GCS,VetiverModels storage;\n    class HurdleModel,WeightModel,RatingModels,Models model;\n    class PrepData,DataSets,NewGames,Predictions data;"
  },
  {
    "objectID": "methodology.html#data",
    "href": "methodology.html#data",
    "title": "Methodology",
    "section": "",
    "text": "I train and evaluate the models primarily on games that have achieved at least 25 ratings, as this is the required number of user ratings for a game to receive a Geek rating.\nThe majority of games on BoardGameGeek do not actually receive this number of ratings, especially in recent years as the number of games published has dramatically increased. My main aim is to predict games that are set to be published and available to purchase, and many of the over 100k games on BGG have not been been published or distributed.\nIdeally, I would have a variable that measures this directly, but I instead rely on achieving a minimum number of ratings as a heuristic.\n\n\nShow the code\nplot_games_by_split = function(data) {\n    plot_data =\n        data |&gt;\n        bggUtils:::unnest_outcomes() |&gt;\n        inner_join(\n            data |&gt;\n                bggUtils:::unnest_info()\n        ) |&gt;\n        add_hurdle() |&gt;\n        mutate(\n            yearpublished = case_when(\n                yearpublished &lt; 1950 ~ 1949,\n                TRUE ~ yearpublished\n            )\n        ) |&gt;\n        mutate(\n            hurdle = case_when(\n                hurdle == 'yes' ~ '&gt;25 ratings',\n                hurdle == 'no' ~ '&lt;25 ratings'\n            )\n        ) |&gt;\n        group_by(yearpublished, hurdle) |&gt;\n        count()\n\n    plot_data |&gt;\n        ggplot(aes(x = yearpublished, y = n, fill = hurdle)) +\n        geom_col() +\n        scale_color_manual(values = c(\"grey60\", \"navy\"))\n}\n\nplot_games_by_split(games) +\n    scale_fill_manual(values = c('coral', 'navy'))\n\n\n\n\n\n\n\n\n\nIn predicting new games, I first use a hurdle model to predict whether games are expected to receive enough ratings to be assigned a Geek rating (25 ratings). This model is trained on the full universe of games, whereas the others are trained on games with at least 25 ratings.\nThe model for predicting the average weight is trained on games that have received 25 ratings and have also received at least 5 votes on their complexity.\n\n\nShow the code\nbind_rows(\n    as_tibble(hurdle_fit$metadata$user$data) |&gt;\n        group_by(model = 'hurdle') |&gt;\n        count(),\n    as_tibble(average_fit$metadata$user$data) |&gt;\n        group_by(model = 'average') |&gt;\n        count(),\n    as_tibble(averageweight_fit$metadata$user$data) |&gt;\n        group_by(model = 'averageweight') |&gt;\n        count(),\n    as_tibble(usersrated_fit$metadata$user$data) |&gt;\n        group_by(model = 'usersrated') |&gt;\n        count()\n) |&gt;\n    ungroup() |&gt;\n    rename(games = n) |&gt;\n    arrange(desc(games)) |&gt;\n    gt::gt()\n\n\n\n\n\n  \n    \n      model\n      games\n    \n  \n  \n    hurdle\n103528\n    average\n27812\n    usersrated\n27812\n    averageweight\n17315"
  },
  {
    "objectID": "methodology.html#assessment",
    "href": "methodology.html#assessment",
    "title": "Methodology",
    "section": "",
    "text": "How did the models perform in predicting games? I evaluated the model on games published in 2022 and 2023.\n\n\nShow the code\nvalid_predictions |&gt;\n    group_by(yearpublished) |&gt;\n    count() |&gt;\n    ungroup() |&gt;\n    gt::gt()\n\n\n\n\n\n  \n    \n      yearpublished\n      n\n    \n  \n  \n    2022\n5053\n    2023\n5205\n  \n  \n  \n\n\n\n\n\n\nI first used the hurdle model to classify whether games would receive at least 25 ratings.\nI set the probability threshold at 0.16 - this was the point that maximized the F2 score and minimized false negatives. For the purpose of the hurdle model, including a game that did not receive a Geek rating is much worse than missing a game that did. The main purpose of the model is filter out low-quality games.\n\nThresholdMetricsConfusion MatrixFalse NegativesFalse Positives\n\n\n\n\nShow the code\nvalid_predictions |&gt;\n    ggplot(aes(x = .pred_hurdle_yes, fill = hurdle)) +\n    geom_density(alpha = 0.5) +\n    scale_color_manual() +\n    theme(legend.title = element_text()) +\n    xlab(\"Pr(User Ratings &gt;= 25)\") +\n    scale_fill_manual(values = c(\"coral\", \"navy\")) +\n    guides(fill = guide_legend(title = 'User Ratings &gt;=25')) +\n    geom_vline(xintercept = hurdle_threshold, linetype = 'dashed')\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nprob_metrics = metric_set(yardstick::roc_auc, yardstick::pr_auc)\n\nprob_hurdle_metrics =\n    valid_predictions |&gt;\n    group_by(outcome = 'hurdle') |&gt;\n    prob_metrics(truth = hurdle, .pred_hurdle_yes, event_level = 'second')\n\nvalid_hurdle_metrics |&gt;\n    bind_rows(prob_hurdle_metrics) |&gt;\n    gt::gt() |&gt;\n    gt::tab_options(quarto.disable_processing = T) |&gt;\n    gt::fmt_number(columns = c(\".estimate\"), decimals = 3) |&gt;\n    gtExtras::gt_theme_espn()\n\n\n\n\n\n  \n    \n      outcome\n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    hurdle\nbal_accuracy\nbinary\n0.763\n    hurdle\nkap\nbinary\n0.424\n    hurdle\nmcc\nbinary\n0.476\n    hurdle\nf1_meas\nbinary\n0.636\n    hurdle\nf2_meas\nbinary\n0.768\n    hurdle\nprecision\nbinary\n0.494\n    hurdle\nrecall\nbinary\n0.891\n    hurdle\nj_index\nbinary\n0.526\n    hurdle\nroc_auc\nbinary\n0.861\n    hurdle\npr_auc\nbinary\n0.737\n  \n  \n  \n\n\n\n\n\n\n\n\nShow the code\nvalid_predictions |&gt;\n    conf_mat(hurdle, .pred_hurdle_class) |&gt;\n    autoplot(type = 'heatmap')\n\n\n\n\n\n\n\n\n\n\n\nWhich games that the model classified as “no” received 25+ ratings?\n\n\nShow the code\nvalid_predictions |&gt;\n    filter(.pred_hurdle_class == 'no') |&gt;\n    filter(usersrated &gt;= 25) |&gt;\n    select(yearpublished, game_id, name, usersrated, bayesaverage) |&gt;\n    arrange(desc(usersrated)) |&gt;\n    mutate(bayesaverage = round(bayesaverage, 3)) |&gt;\n    mutate(across(all_of(c(\"yearpublished\", \"game_id\")), ~ as.factor(.x))) |&gt;\n    DT::datatable()\n\n\n\n\n\n\n\n\nWhich games that the model classified as “yes” did not receive 25+ ratings?\n\n\nShow the code\nvalid_predictions |&gt;\n    filter(.pred_hurdle_class == 'yes') |&gt;\n    filter(usersrated &lt; 25) |&gt;\n    select(yearpublished, game_id, name, usersrated, .pred_bayesaverage) |&gt;\n    arrange(desc(usersrated)) |&gt;\n    mutate(.pred_bayesaverage = round(.pred_bayesaverage, 3)) |&gt;\n    mutate(across(all_of(c(\"yearpublished\", \"game_id\")), ~ as.factor(.x))) |&gt;\n    DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\n\nI evaluate predictions for the average weight rating, the average rating, the number of user ratings, and the Geek rating.\n\nGames with &gt;25 RatingsGames with &lt;25 Ratings\n\n\n\n\nShow the code\nplot_hurdle_yes\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplot_hurdle_no\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ntargets_tracking_details(metrics = valid_metrics, details = details) |&gt;\n    select(\n        model,\n        minratings,\n        outcome,\n        any_of(c(\"rmse\", \"mae\", \"mape\", \"rsq\", \"ccc\"))\n    ) |&gt;\n    filter(minratings == 25) |&gt;\n    select(minratings, everything()) |&gt;\n    gt::gt() |&gt;\n    gt::tab_options(quarto.disable_processing = T) |&gt;\n    gtExtras::gt_theme_espn()\n\n\n\n\n\n  \n    \n      minratings\n      model\n      outcome\n      rmse\n      mae\n      mape\n      rsq\n      ccc\n    \n  \n  \n    25\nglmnet\naverage\n0.675\n0.498\n7.374\n0.294\n0.487\n    25\nlightgbm\naverageweight\n0.437\n0.336\n18.019\n0.706\n0.827\n    25\nglmnet+glmnet\nbayesaverage\n0.285\n0.159\n2.647\n0.430\n0.649\n    25\nglmnet\nusersrated\n1941.387\n446.031\n154.763\n0.122\n0.335\n  \n  \n  \n\n\n\n\nWhat were the model’s top predictions in the validation set?\n\n\nShow the code\nvalid_predictions |&gt;\n    filter(.pred_hurdle_class == 'yes') |&gt;\n    select(-starts_with(\".pred_hurdle\")) |&gt;\n    slice_max(.pred_bayesaverage, n = 150, with_ties = F) |&gt;\n    predictions_dt(\n        games = games,\n        lazy_load = TRUE,\n        pageLength = 10\n    ) |&gt;\n    add_colors()"
  },
  {
    "objectID": "methodology.html#features",
    "href": "methodology.html#features",
    "title": "Methodology",
    "section": "",
    "text": "Which features were influential for predicting each BGG outcome?\n\n\nShow the code\naverage_plot =\n    average_fit |&gt;\n    extract_vetiver_features() |&gt;\n    plot_model_features() +\n    labs(title = 'Average Rating')\n\naverageweight_plot =\n    averageweight_fit |&gt;\n    extract_vetiver_features() |&gt;\n    plot_model_features() +\n    labs(title = 'Average Weight')\n\nusersrated_plot =\n    usersrated_fit |&gt;\n    extract_vetiver_features() |&gt;\n    plot_model_features() +\n    labs(title = 'Users Rated')\n\n\n\nAverage WeightAverageUsers Rated"
  }
]