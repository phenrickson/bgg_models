[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting Upcoming Board Games",
    "section": "",
    "text": "I use historical data from BoardGameGeek (BGG) to train a number of predictive models for community ratings. I first classify games based on their probability of achieving a minimum number of ratings on BGG. I then estimate each gameâ€™s complexity (average weight) in order to predicts its number of user ratings and average rating. I then use these estimates to predict the expected Geek Rating.\nThe following (somewhat messy) visualization displays the status of the current pipeline used to train models and predict new games.\n\n\nShow the code\ntargets::tar_visnetwork(targets_only =T)\n\n\n\n\n\n\n\n\n\nShow the code\nmodel_board = gcs_model_board(bucket = config$bucket, prefix = config$board)\n\ntar_load(averageweight_vetiver)\ntar_load(average_vetiver)\ntar_load(usersrated_vetiver)\ntar_load(hurdle_vetiver)\ntar_load(hurdle_threshold)\n\naverageweight_fit =\n    pin_read_model(model_board,\n                   averageweight_vetiver)\n\naverage_fit =\n    pin_read_model(model_board,\n                   average_vetiver)\n\nusersrated_fit =\n    pin_read_model(model_board,\n                   usersrated_vetiver)\n\nhurdle_fit =\n    pin_read_model(model_board,\n                   hurdle_vetiver)\n\n\n\n\nShow the code\nend_valid_year = valid_years$max_year \n\nupcoming_games = \n    active_games |&gt;\n    filter(yearpublished &gt; end_valid_year)\n\n\n\n\nHow did the models perform in predicting games?\nI used a training-validation approach based around the year in which games were published. I creating a training set of games published prior to 2022 and evaluated its performance in predicting games published from 2022 to 2023. \n\n\nHow did the model perform in predicting new games? I evaluate the model primarily on games that achieved at least 25 ratings.\n\nGames with &gt;25 RatingsGames with &lt;25 Ratings\n\n\n\n\nShow the code\nplot_hurdle_yes\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplot_hurdle_no\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ntargets_tracking_details(metrics = valid_metrics,\n                         details = details) |&gt;\n    select(model, minratings, outcome, any_of(c(\"rmse\", \"mae\", \"mape\", \"rsq\", \"ccc\"))) |&gt;\n    filter(minratings == 25) |&gt;\n    select(minratings, everything()) |&gt;\n    gt::gt() |&gt;\n    gt::tab_options(quarto.disable_processing = T) |&gt;\n    gtExtras::gt_theme_espn()\n\n\n\n\n\n  \n    \n      minratings\n      model\n      outcome\n      rmse\n      mae\n      mape\n      rsq\n      ccc\n    \n  \n  \n    25\nglmnet\naverage\n0.675\n0.498\n7.374\n0.294\n0.487\n    25\nlightgbm\naverageweight\n0.437\n0.336\n18.019\n0.706\n0.827\n    25\nglmnet+glmnet\nbayesaverage\n0.285\n0.159\n2.647\n0.430\n0.649\n    25\nglmnet\nusersrated\n1941.387\n446.031\n154.763\n0.122\n0.335\n  \n  \n  \n\n\n\n\nWhat were the top predictions in the validation set?\n\n\nShow the code\nvalid_predictions |&gt;\n    filter(.pred_hurdle_class == 'yes') |&gt;\n    select(-starts_with(\".pred_hurdle\")) |&gt;\n    slice_max(.pred_bayesaverage, n =150, with_ties = F) |&gt;\n    predictions_dt(games = games) |&gt;\n    add_colors()\n\n\n\n\n\n\n\n\n\nI use a hurdle model to predict whether games are expected to receive enough ratings to be assigned a geek rating (25 ratings). This is a classification model which assigns a probability to a game; in order to classify games, I need to determine the appropriate threshold.\nI set the threshold at 0.16 - this is the point that maximizes the (F2 measure) and minimizes false negatives. For the purpose of this model, including a game that did not receive a Geek rating is much worse than missing a game that did.\n\nThresholdMetricsConfusion Matrix\n\n\n\n\nShow the code\nvalid_predictions |&gt; \n    ggplot(aes(x=.pred_hurdle_yes, fill = hurdle))+\n    geom_density(alpha = 0.5)+\n    scale_color_manual()+\n    theme(legend.title = element_text())+\n    xlab(\"Pr(User Ratings &gt;= 25)\")+\n    scale_fill_manual(values = c(\"coral\", \"navy\"))+\n    guides(fill = guide_legend(title = 'User Ratings &gt;=25'))+\n    geom_vline(xintercept = hurdle_threshold,\n               linetype = 'dotted')\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nprob_metrics = metric_set(yardstick::roc_auc,\n                          yardstick::pr_auc)\n\nprob_hurdle_metrics = \n    valid_predictions |&gt;\n    group_by(outcome = 'hurdle') |&gt;\n    prob_metrics(truth = hurdle,\n                 .pred_hurdle_yes,\n                 event_level = 'second')\n\nvalid_hurdle_metrics |&gt;\n    bind_rows(prob_hurdle_metrics) |&gt;\n    gt::gt() |&gt;\n    gt::tab_options(quarto.disable_processing = T) |&gt;\n    gt::fmt_number(columns = c(\".estimate\"),\n                   decimals = 3) |&gt;\n    gtExtras::gt_theme_espn()\n\n\n\n\n\n  \n    \n      outcome\n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    hurdle\nbal_accuracy\nbinary\n0.763\n    hurdle\nkap\nbinary\n0.424\n    hurdle\nmcc\nbinary\n0.476\n    hurdle\nf1_meas\nbinary\n0.636\n    hurdle\nf2_meas\nbinary\n0.768\n    hurdle\nprecision\nbinary\n0.494\n    hurdle\nrecall\nbinary\n0.891\n    hurdle\nj_index\nbinary\n0.526\n    hurdle\nroc_auc\nbinary\n0.861\n    hurdle\npr_auc\nbinary\n0.737\n  \n  \n  \n\n\n\n\n\n\n\n\nShow the code\nvalid_predictions |&gt;\n    conf_mat(hurdle, .pred_hurdle_class) |&gt;\n    autoplot(type = 'heatmap')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich features were influential for predicting each BGG outcome?\n\n\nShow the code\naverage_plot = \n    average_fit |&gt; \n    extract_vetiver_features() |&gt;\n    plot_model_features()+\n    labs(title = 'Average Rating')\n\naverageweight_plot = \n    averageweight_fit |&gt; \n    extract_vetiver_features() |&gt;\n    plot_model_features()+\n    labs(title = 'Average Weight')\n\nusersrated_plot = \n    usersrated_fit |&gt; \n    extract_vetiver_features() |&gt;\n    plot_model_features()+\n    labs(title = 'Users Rated')\n\n\n\nAverage WeightAverageUsers Rated"
  },
  {
    "objectID": "index.html#assessment",
    "href": "index.html#assessment",
    "title": "Predicting Upcoming Board Games",
    "section": "",
    "text": "How did the models perform in predicting games?\nI used a training-validation approach based around the year in which games were published. I creating a training set of games published prior to 2022 and evaluated its performance in predicting games published from 2022 to 2023. \n\n\nHow did the model perform in predicting new games? I evaluate the model primarily on games that achieved at least 25 ratings.\n\nGames with &gt;25 RatingsGames with &lt;25 Ratings\n\n\n\n\nShow the code\nplot_hurdle_yes\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplot_hurdle_no\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ntargets_tracking_details(metrics = valid_metrics,\n                         details = details) |&gt;\n    select(model, minratings, outcome, any_of(c(\"rmse\", \"mae\", \"mape\", \"rsq\", \"ccc\"))) |&gt;\n    filter(minratings == 25) |&gt;\n    select(minratings, everything()) |&gt;\n    gt::gt() |&gt;\n    gt::tab_options(quarto.disable_processing = T) |&gt;\n    gtExtras::gt_theme_espn()\n\n\n\n\n\n  \n    \n      minratings\n      model\n      outcome\n      rmse\n      mae\n      mape\n      rsq\n      ccc\n    \n  \n  \n    25\nglmnet\naverage\n0.675\n0.498\n7.374\n0.294\n0.487\n    25\nlightgbm\naverageweight\n0.437\n0.336\n18.019\n0.706\n0.827\n    25\nglmnet+glmnet\nbayesaverage\n0.285\n0.159\n2.647\n0.430\n0.649\n    25\nglmnet\nusersrated\n1941.387\n446.031\n154.763\n0.122\n0.335\n  \n  \n  \n\n\n\n\nWhat were the top predictions in the validation set?\n\n\nShow the code\nvalid_predictions |&gt;\n    filter(.pred_hurdle_class == 'yes') |&gt;\n    select(-starts_with(\".pred_hurdle\")) |&gt;\n    slice_max(.pred_bayesaverage, n =150, with_ties = F) |&gt;\n    predictions_dt(games = games) |&gt;\n    add_colors()\n\n\n\n\n\n\n\n\n\nI use a hurdle model to predict whether games are expected to receive enough ratings to be assigned a geek rating (25 ratings). This is a classification model which assigns a probability to a game; in order to classify games, I need to determine the appropriate threshold.\nI set the threshold at 0.16 - this is the point that maximizes the (F2 measure) and minimizes false negatives. For the purpose of this model, including a game that did not receive a Geek rating is much worse than missing a game that did.\n\nThresholdMetricsConfusion Matrix\n\n\n\n\nShow the code\nvalid_predictions |&gt; \n    ggplot(aes(x=.pred_hurdle_yes, fill = hurdle))+\n    geom_density(alpha = 0.5)+\n    scale_color_manual()+\n    theme(legend.title = element_text())+\n    xlab(\"Pr(User Ratings &gt;= 25)\")+\n    scale_fill_manual(values = c(\"coral\", \"navy\"))+\n    guides(fill = guide_legend(title = 'User Ratings &gt;=25'))+\n    geom_vline(xintercept = hurdle_threshold,\n               linetype = 'dotted')\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nprob_metrics = metric_set(yardstick::roc_auc,\n                          yardstick::pr_auc)\n\nprob_hurdle_metrics = \n    valid_predictions |&gt;\n    group_by(outcome = 'hurdle') |&gt;\n    prob_metrics(truth = hurdle,\n                 .pred_hurdle_yes,\n                 event_level = 'second')\n\nvalid_hurdle_metrics |&gt;\n    bind_rows(prob_hurdle_metrics) |&gt;\n    gt::gt() |&gt;\n    gt::tab_options(quarto.disable_processing = T) |&gt;\n    gt::fmt_number(columns = c(\".estimate\"),\n                   decimals = 3) |&gt;\n    gtExtras::gt_theme_espn()\n\n\n\n\n\n  \n    \n      outcome\n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    hurdle\nbal_accuracy\nbinary\n0.763\n    hurdle\nkap\nbinary\n0.424\n    hurdle\nmcc\nbinary\n0.476\n    hurdle\nf1_meas\nbinary\n0.636\n    hurdle\nf2_meas\nbinary\n0.768\n    hurdle\nprecision\nbinary\n0.494\n    hurdle\nrecall\nbinary\n0.891\n    hurdle\nj_index\nbinary\n0.526\n    hurdle\nroc_auc\nbinary\n0.861\n    hurdle\npr_auc\nbinary\n0.737\n  \n  \n  \n\n\n\n\n\n\n\n\nShow the code\nvalid_predictions |&gt;\n    conf_mat(hurdle, .pred_hurdle_class) |&gt;\n    autoplot(type = 'heatmap')"
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Predicting Upcoming Board Games",
    "section": "",
    "text": "Which features were influential for predicting each BGG outcome?\n\n\nShow the code\naverage_plot = \n    average_fit |&gt; \n    extract_vetiver_features() |&gt;\n    plot_model_features()+\n    labs(title = 'Average Rating')\n\naverageweight_plot = \n    averageweight_fit |&gt; \n    extract_vetiver_features() |&gt;\n    plot_model_features()+\n    labs(title = 'Average Weight')\n\nusersrated_plot = \n    usersrated_fit |&gt; \n    extract_vetiver_features() |&gt;\n    plot_model_features()+\n    labs(title = 'Users Rated')\n\n\n\nAverage WeightAverageUsers Rated"
  },
  {
    "objectID": "index.html#upcoming-games",
    "href": "index.html#upcoming-games",
    "title": "Predicting Upcoming Board Games",
    "section": "2.1 Upcoming Games",
    "text": "2.1 Upcoming Games\nThe following table displays predicted BGG outcomes for games that are expected to achieve at least 25 user ratings."
  },
  {
    "objectID": "index.html#hurdle-1",
    "href": "index.html#hurdle-1",
    "title": "Predicting Upcoming Board Games",
    "section": "2.2 Hurdle",
    "text": "2.2 Hurdle\nThis table displays predicted probabilities for whether games will achieve enough ratings (25) to be assigned a Geek Rating."
  }
]