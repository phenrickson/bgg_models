---
title: "Predicting User Collections"
author: "Phil Henrickson"
date: "`r Sys.Date()`"
output: html_document
params:
        bgg_username: "mrbananagrabber"
        code_folding: "hide"
---

# What is this? {-}

This notebook contains a set of analyses for analyzing **`r get_username()`'s** BoardGameGeek collection. The bulk of the analysis is focused on building a user-specific predictive model to predict the games that the specified user is likely to add to their collection. 

By analyzing a user's collection and training a predictive model, I am able to answer questions such as:

- What designers/mechanics/genres does a user tend to like or dislike?

- What older games might they be interested in adding to their collection?

- What new and upcoming games should they check out?

```{r knitr setup, include =F}

source(here::here("src", "reports", "knitr_settings.R"))

set_knitr_options()

theme_bgg = theme_bgg() +
        theme(plot.title = element_text(vjust=2))


```

# Data

## Outcomes

How many games has `r get_username()` owned/rated/played?

<!-- We can look at the most frequent types of categories, mechanics, designers, and artists that appear in a user's collection. -->

```{r owned plot}

get_collection() %>%
        collection_owned_plot()+
        theme(plot.title = element_text(vjust=2))
```

## Collection

What types of game does `r get_username()` own? I can look at the most frequent types of categories, mechanics, designers, and artists that appear in a user's collection.

```{r categories plot, fig.height = 8}

get_collection() %>%
        collection_categories_plot()
```

## Collection Table

What games does **`r params$bgg_username`** currently have in their collection? The following table can be used to examine games the user owns, along with some helpful information for selecting the right game for a game night!

Use the filters above the table to sort/filter based on information about the game, such as year published, recommended player counts, or playing time.

```{r collection gt table}

get_collection() %>%
        filter(own == 'yes') %>%
        arrange(desc(rating)) %>%
        prep_collection_table() %>%
        make_collection_datable()

```

# Modeling

I'll now the examine predictive models trained on the user's collection. 

For an individual user, I train a predictive model on their collection in order to predict **whether a user owns a game**. The outcome, in this case, is binary: does the user have a game listed in their collection or not? This is the setting for training a classification model, where the model aims to **learn the probability** that a user will add a game to their collection based on its observable features.

How does a model learn what a user is likely to own? The training process is a matter of examining historical games and finding patterns that exist between game features (designers, mechanics, playing time, etc) and games in the user's collection. 

Note: I train models to predict whether a user owns a game based only on information that could be observed about the game at its release: playing time, player count, mechanics, categories, genres, and selected designers, artists, and publishers. I do not make use of BGG community information, such as its average rating or number of user ratings (though I do use a game's estimated complexity as a feature). This is to ensure the model can predict newly released games and is not dependent on the BGG community to rate them.

## What Predicts a User's Collection?

A predictive model gives us more than just predictions. We can also ask, what did the model learn from the data? What predicts the outcome? In the case of predicting a boardgame collection, what did the model find to be predictive of games a user owns?

To answer this, I can examine the **coefficients** from a model logistic regression with ridge regularization (which I will refer to as a penalized logistic regression). Positive values indicate that a feature increases a user's probability of owning/rating a game, while negative values indicate a feature decreases the probability. To be precise, the coefficients indicate the effect of a particular feature on the log-odds of a user owning a game.

```{r coef plot, fig.height = 8}

coef_plot = user_workflows %>%
        get_workflow(model = "glmnet") %>%
        glmnet_coef_plot()+
        geom_col(color = 'white') +
        labs(title = paste("What Predicts", get_username(), "'s Collection?", sep=" "),
             subtitle = str_wrap("Coefficients from a penalized (ridge) logistic regression for games owned by specified user. Predictors centered and scaled."))+
        theme(plot.title = element_text(hjust = 0,
                                        size = 12,
                                        vjust = 2),
              plot.subtitle = element_text(hjust = 0,
                                           size = 10))+
        my_caption()+
        xlab("Estimated Effect on Outcome")+
        ylab("Features")

# get range for later visualizations
coef_range = layer_scales(coef_plot)$x$range$range

coef_plot

```

This model examines a wide variety of features of games (`r get_workflow(user_workflows, "glmnet") %>% extract_mold() %$% predictors %>% ncol()` features, to be exact) and estimates their effect on whether a user owns a game. These estimates are then shrunken towards zero based on a tuning parameter (lambda), where the appropriate value is estimated from the data.

The following visualization shows the path of each feature as it enters the model, with highly influential features tending to enter the model early with large positive or negative effects.

```{r trace plot}

trace = user_workflows %>%
        get_workflow(model = "glmnet") %>%
        glmnet_trace_plot() +
        theme(plot.subtitle = element_text(size = 8,
                                           hjust = 0))
        # labs(title = paste0('Coefficient trace plot from ridge regression',
        #                     '\n',
        #                     'dotted line indicates selected level of regularization'))

suppressWarnings({
        print(trace)
        
})

```

## Partial Effects

This type of model enables me to I can examine the effects of specific features on a user's collection. For instance, what is a user's favorite designer? Least favorite mechanic? The following plots indicate specific effects for different kinds of features.

```{r examine effects, fig.height = 8}

effects_plots = 
        map(c("designer", "mechanic", "publisher", "category"),
            ~ user_workflows %>%
                    get_workflow(model = "glmnet") %>%
                    extract_fit_parsnip() %>%
                    tidy() %>%
                    make_effects_plot(category = .x)+
                    xlim(coef_range[1]-0.01,
                         coef_range[2]+0.01)
        )

# combine
cowplot::plot_grid(plotlist=effects_plots)


# # designers
# user_workflows %>%
#         get_workflow(model = "glmnet") %>%
#         extract_fit_parsnip() %>%
#         tidy() %>%
#         make_effects_plot(category = 'designer')+
#         xlim(coef_range[1],
#              coef_range[2])
# 
# # mechanics
# user_workflows %>%
#         get_workflow(model = "glmnet") %>%
#         extract_fit_parsnip() %>%
#         tidy() %>%
#         make_effects_plot(category = 'mechanic') +
#         xlim(coef_range[1],
#              coef_range[2])
# 
# # publisher
# user_workflows %>%
#         get_workflow(model = "glmnet") %>%
#         extract_fit_parsnip() %>%
#         tidy() %>%
#         make_effects_plot(category = 'publisher') +
#         xlim(coef_range[1],
#              coef_range[2])
# 
# # categories
# user_workflows %>%
#         get_workflow(model = "glmnet") %>%
#         extract_fit_parsnip() %>%
#         tidy() %>%
#         make_effects_plot(category = 'category') +
#         xlim(coef_range[1],
#              coef_range[2])

```

## Feature Importance

In addition to training a logistic regression, I trained another type of model using boosted trees (LightGBM), a flexible nonparametric method that is well suited for prediction. 

Which features were most used by this model? Features that are important in predicting a user's collection will appear towards the top of cover, frequency, and/or gain.

```{r lightgbm variable importance, fig.height = 8}

user_workflows %>%
        get_workflow(model = "lightgbm") %>%
        lightgbm_vip() %>%
        lightgbm_vip_plot()+
        labs(
                title = paste("What Predicts", get_username(), "'s Collection?", sep=" "),
                subtitle = str_wrap("Feature importance from light gradient boosted machines, an ensemble of boosted decision trees."))+
        theme(plot.title = element_text(hjust = 0,
                                        size = 12),
              plot.subtitle = element_text(hjust = 0,
                                           size = 10))+
        my_caption()+
        xlab("Value")+
        ylab("Feature")+
        theme(panel.grid.major = element_blank())

```

# Assessment

How well did the model do in predicting the user's collection?

This section contains a variety of visualizations and metrics for assessing the performance of the model(s). If you're not particularly interested in predictive modeling, skip down further to the predictions from the model.

## Separation

An easy way to examine the performance of classification model is to view a separation plot. 

I plot the predicted probabilities from the model for every game (from resampling) from lowest to highest. We then overlay a blue line for any game that the user does own. A good classifier is one that is able to *separate* the blue (games owned by the user) from the white (games not owned by the user), with most of the blue occurring at the highest probabilities (right side of the chart).

```{r separation plots, fig.height = 6}


user_results$training_predictions %>%
        make_separation_plot(truth = user_results$outcome)+
        labs(title = paste("How well did the models do?"),
             subtitle = str_wrap("Displaying cross validated probabilities for all games in the training set from least likely to most likely. Vertical blue lines indicate game was actually in the user's collection.", 125))+
        theme(plot.title = element_text(hjust = 0,
                                        size = 12,
                                        vjust = 2),
              plot.subtitle = element_text(hjust = 0,
                                           size = 10))

```

I can more formally assess how well each model did in resampling by looking at the *area under the receiver operating characteristic curve* (roc_auc). A perfect model would receive a score of 1, while a model that cannot predict the outcome will default to a score of 0.5. The extent to which something is a *good* score depends on the setting, but generally anything in the .8 to .9 range is very good while the .7 to .8 range is perfectly acceptable. 

```{r training assessment}

user_results$training_metrics %>%
        mutate(wflow_id = tidy_wflow_id(wflow_id)) %>%
        select(type, wflow_id, .metric, mean, std_err, n) %>%
        mutate_if(is.numeric, round, 3) %>%
        filter(.metric == 'roc_auc') %>%
        gt() %>%
        add_gt_wflow_color() %>%
        set_gt_tab_options() %>%
        tab_options(container.height = 150)

```

```{r training roc}

user_results$training_predictions %>%
        make_roc_plot(truth = user_results$outcome)

```

## Top Games in Training

```{r}

older_games = 
        user_output$training_predictions %>%
        filter(wflow_id == 'all_trees_lightgbm')

# new and upcoming
older_games %>%
        make_predictions_gt_table(
                outcome = user_results$outcome,
                top_n = 10,
                description_length = 400
        ) %>%
        gt::tab_header(
                title = paste("Top (Older) Games for",
                              get_username()),
                subtitle = paste("Rankings based on predictive model trained on user's collection using games released through",
                                 user_output$end_train_year)
        ) %>%
        set_gt_tab_options() %>%
        gt::tab_options(
                table.font.size = 10
        )

```

Top 10 games most likely to be owned by the user in the last 10 years of the training set, highlighting in blue games that are currently in the user's collection and light blue for games that the user previously owned.

```{r top n table training}

older_games %>%
        make_top_n_table(
                workflow = 'all_trees_lightgbm',
                outcome = user_output$outcome,
                years = seq(user_output$end_train_year -10,
                            user_output$end_train_year),
                top_n = 10
        ) %>%
        gt::tab_header(
                title = paste("Top Games by Year for",
                              get_username()),
                subtitle = paste("Rankings based on predictive model trained on user's collection using games released through", user_output$end_train_year)
        ) %>%
        set_gt_theme() %>%
        set_gt_tab_options() %>%
        gt::tab_options(table.font.size = 10) %>%
        tab_options(container.height = 600)

```

## Calibration 

What do the model's predicted probabilties mean? Or, put another way, how well *calibrated* are the model's predictions?

If the model assigns a probability of 5%, how often does the outcome actually occur? A well calibrated model is one in which the predicted probabilities reflect the probabilities we would observe in the actual data. We can assess the calibration of a model by grouping its predictions into bins and assessing how often we observe the outcome versus how often each model expects to observe the outcome.

A model that is well calibrated will closely follow the dashed line - its expected probabilities match that of the observed probabilities. A model that consistently underestimates the probability of the event will be over this dashed line, be a while a model that overestimates the probability will be under the dashed line.

```{r}

wflows = user_output$training_predictions %>%
        pull(wflow_id) %>%
        unique()

calib_plots = map(wflows,
                  ~ user_output$training_predictions %>%
                          filter(wflow_id == .x) %>%
                          mutate(wflow_id = tidy_wflow_id(wflow_id)) %>%
                          probably::cal_plot_breaks(
                                  group = wflow_id,
                                  truth = own,
                                  estimate = .pred_yes,
                                  event_level = 'second')+
                          facet_wrap(wflow_id ~.))

cowplot::plot_grid(plotlist=calib_plots)
```

## Validation

I first assessed the models based on their performance via resampling on the training set. 

But how well does my modeling approach do in predicting *new* games? To test this, I assessed the performance of the model (which was trained on games published through `r user_results$end_train_year`) on games published in `r paste(user_results$end_train_year  +1, user_results$end_train_year + valid_window, sep = "-")`.

How well did the model do? The following table shows the model's predictions for games in the validation set.

```{r valid predictions}

user_results$valid_predictions %>%
        prep_games_datatable() %>%
        make_games_datatable()

# # new and upcoming
# older_games %>%
#         make_predictions_gt_table(
#                 outcome = user_results$outcome,
#                 top_n = 10,
#                 description_length = 400
#         ) %>%
#         gt::tab_header(
#                 title = paste("Top (Older) Games for",
#                               get_username()),
#                 subtitle = paste("Rankings based on predictive model trained on user's collection using games released through",
#                                  user_output$end_train_year)
#         ) %>%
#         set_gt_tab_options() %>%
#         gt::tab_options(
#                 table.font.size = 10
#         )

```

As before, I can then assess the performance of the model.

```{r validation metrics}

user_results$valid_metrics %>%
        mutate(wflow_id = tidy_wflow_id(wflow_id)) %>%
        select(type, wflow_id, .metric, .estimate) %>%
        mutate_if(is.numeric, round, 3) %>%
        arrange(.metric, wflow_id) %>%
        gt() %>%
        add_gt_wflow_color() %>%
        set_gt_tab_options() %>%
        tab_options(container.height = 200)

```

```{r validation roc}

user_results$valid_predictions %>%
        make_roc_plot(truth = user_results$outcome)

```

# Predictions

What new and upcoming games does the model predict for **`r get_username()`**?

```{r set top n}

top_games = 15
        
```

The following table displays the top `r top_games` games published after `r user_results$end_train_year` with the highest probability of entering the user's collection.

```{r}

newer_games = 
        bind_rows(
                user_output$valid_predictions,
                user_output$upcoming_predictions %>%
                        filter(yearpublished > user_output$end_train_year + valid_window)) %>%
        filter(wflow_id == 'all_trees_lightgbm')

newer_games %>%
        make_predictions_gt_table(
                outcome = 'own',
                top_n = top_games,
                description_length = 400
        ) %>%
        gt::tab_header(
                title = paste("Top", top_games, "(Newer) Games for",
                              get_username()),
                subtitle = paste("Rankings based on predictive model trained on user's collection using games released through",
                                 user_output$end_train_year)
        ) %>%
        set_gt_tab_options() %>%
        gt::tab_options(
                table.font.size = 12
        )

```


## Explaining Individual Predictions

Why did the model predict these games? 

```{r shap, warning =F, message =F, fig.height = 6}

# select top 6 games for 2023
ids =
        user_output$upcoming_predictions %>%
        filter(grepl("lightgbm", wflow_id)) %>%
        head(4) %>%
        pull(game_id)

# make plots
explain_plots = map(ids,
                    ~ get_game_data(game_data = user_output$user_collection %>%
                                            join_bgg_games(games),
                                    id = .x) %>%
                            lightgbm_interpret(game_data = .,
                                               outcome = user_output$outcome) %>%
                            lightgbm_interpret_plot(outcome = user_output$outcome,
                                                    n_features = 15)+
                            theme(plot.title = element_text(hjust = 0,
                                                            size = 8,
                                                            vjust = 2),
                                  plot.subtitle = element_text(hjust = 0,
                                                               size = 8),
                                  strip.text.x = element_text(size = 8))
)

# walk(explain_plots,
#      plot)

# # game plots
# library(cowplot)
# library(magick)
# 
# game_plots = map2(.x = games_info %>%
#                           select(game_id, name, bgg_info, images) %>%
#                           unnest(c(bgg_info, images)),
#                   .y = c(12),
#                   ~ cowplot::ggdraw())


# 
# 
#                           cowplot::draw_image(.x %>%
#                                                       filter(game_id .y) %>%
#                                                       pull(image)))
# 
# 
#                           cowplot::draw_label(.x %>%
#                                                       filter(game_id == .y) %>%
#                                                       mutate(label = paste0(name,
#                                                                             "(",
#                                                                             yearpublished,
#                                                                             ")")) %>%
#                                                       pull(label)))
# )
#                   # hjust = 0.5,
#                   # size = 14,
#                   # fontfamily = 'mono',
#                   # fontface = 'bold',
#                   # x = 0.4,
#                   # y = 0.15)
# 
# cowplot::ggdraw() + draw_image(, 
#                            scale = 0.5, 
#                            x=-0.1)+
#         draw_label(teams_raw %>%
#                           filter(TEAM == params$team) %>%                 
#                           mutate(name = paste(TEAM, "\n", TEAM_MASCOT)) %>%
#                           pull(name),
#                   hjust = 0.5,
#                   size = 14,
#                   fontfamily = 'mono',
#                   fontface = 'bold',
#                   color = muted(team_color),
#                   x = 0.4,
#                   y = 0.15)
#                          
# 
cowplot::plot_grid(plotlist=explain_plots)

```

## Upcoming Games

Finally, I can examine predictions for all newer and upcoming games.

```{r new and upcoming games datatable, warning = F, message=F}

newer_games %>%
        prep_games_datatable() %>%
        make_games_datatable()

```

